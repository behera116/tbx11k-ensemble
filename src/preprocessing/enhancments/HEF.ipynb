{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8573121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.optimizers import adam_v2\n",
    "from keras import backend as keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "import cv2\n",
    "from cv2 import imread, createCLAHE # read and equalize images\n",
    "\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np\n",
    "from skimage.util import montage as montage2d\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c204fc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /home/TBX11K/HEF\n",
    "\n",
    "!mkdir /home/TBX11K/HEF/train\n",
    "!mkdir /home/TBX11K/HEF/val\n",
    "!mkdir /home/TBX11K/HEF/test\n",
    "\n",
    "!mkdir /home/TBX11K/HEF/train/sick\n",
    "!mkdir /home/TBX11K/HEF/train/health\n",
    "!mkdir /home/TBX11K/HEF/train/tb\n",
    "\n",
    "!mkdir /home/TBX11K/HEF/val/sick\n",
    "!mkdir /home/TBX11K/HEF/val/health\n",
    "!mkdir /home/TBX11K/HEF/val/tb\n",
    "\n",
    "\n",
    "!mkdir /home/TBX11K/HEF/test/Montgomery\n",
    "!mkdir /home/TBX11K/HEF/test/Montgomery/health\n",
    "!mkdir /home/TBX11K/HEF/test/Montgomery/tb\n",
    "!mkdir /home/TBX11K/HEF/test/Montgomery/sick\n",
    "\n",
    "!mkdir /home/TBX11K/HEF/test/Shenzhen\n",
    "!mkdir /home/TBX11K/HEF/test/Shenzhen/health\n",
    "!mkdir /home/TBX11K/HEF/test/Shenzhen/tb\n",
    "!mkdir /home/TBX11K/HEF/test/Shenzhen/sick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ce0f699",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /home/TBX11K/HEF/test/DA_DB\n",
    "!mkdir /home/TBX11K/HEF/test/DA_DB/health\n",
    "!mkdir /home/TBX11K/HEF/test/DA_DB/tb\n",
    "!mkdir /home/TBX11K/HEF/test/DA_DB/sick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57feaee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "def histogram(data):\n",
    "    '''Generates the histogram for the given data.\n",
    "    Parameters:\n",
    "        data: data to make the histogram.\n",
    "    Returns: histogram, bins.\n",
    "    '''\n",
    "\n",
    "    pixels, count = np.unique(data, return_counts=True)\n",
    "    hist = OrderedDict()\n",
    "\n",
    "    for i in range(len(pixels)):\n",
    "        hist[pixels[i]] = count[i]\n",
    "\n",
    "    return np.array(list(hist.values())), np.array(list(hist.keys()))\n",
    "\n",
    "def convertToHEF(img):\n",
    "    '''Runs the algorithm for the image.'''\n",
    "\n",
    "    # HF part\n",
    "    img_fft = np.fft.fft2(img)  # img after fourier transformation\n",
    "    img_sfft = np.fft.fftshift(img_fft)  # img after shifting component to the center\n",
    "\n",
    "    m, n = img_sfft.shape\n",
    "    filter_array = np.zeros((m, n))\n",
    "\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            filter_array[i, j] = 1.0 - np.exp(- ((i-m / 2.0) ** 2 + (j-n / 2.0) ** 2) / (2 * (150 ** 2))) #change limit 80\n",
    "\n",
    "    high_filter = 0.5 + 0.75*filter_array\n",
    "\n",
    "    img_filtered = high_filter * img_sfft\n",
    "    img_hef = np.real(np.fft.ifft2(np.fft.fftshift(img_filtered)))  # HFE filtering done\n",
    "\n",
    "    # HE part\n",
    "    # Building the histogram\n",
    "    hist, bins = histogram(img_hef)\n",
    "    # Calculating probability for each pixel\n",
    "    pixel_probability = hist / hist.sum()\n",
    "    # Calculating the CDF (Cumulative Distribution Function)\n",
    "    cdf = np.cumsum(pixel_probability)\n",
    "    cdf_normalized = cdf * 255\n",
    "    hist_eq = {}\n",
    "    for i in range(len(cdf)):\n",
    "        hist_eq[bins[i]] = int(cdf_normalized[i])\n",
    "\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            img[i][j] = hist_eq[img_hef[i][j]]\n",
    "\n",
    "    return img.astype(np.uint8)\n",
    "\n",
    "\n",
    "\n",
    "def process_clahe(res):\n",
    "    filename = res['filename']\n",
    "    dest_file = res['dest_file']\n",
    "    image = cv2.imread(filename,cv2.IMREAD_GRAYSCALE)\n",
    "    hef_image = convertToHEF(image)\n",
    "    cv2.imwrite(dest_file,hef_image)\n",
    "    return True\n",
    "\n",
    "def process_image_files(img_files,dest_dir):\n",
    "    res = []\n",
    "    for filename in img_files:\n",
    "        dest_file = os.path.join(dest_dir,os.path.basename(filename))\n",
    "        row = {'filename':filename,'dest_file':dest_file}\n",
    "        res.append(row)\n",
    "    \n",
    "    workers = os.cpu_count()\n",
    "    with Pool(workers) as p:\n",
    "        p.map(process_clahe, res)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c77bb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "TBX_DIR = '/home/TBX11K'\n",
    "TBX_DEST_DIR = '/home/TBX11K/HEF'\n",
    "for split_dir in ['train','val']:\n",
    "    for class_dir in ['health','sick','tb']:\n",
    "        file_path = os.path.join(TBX_DIR,split_dir,class_dir,'*.png')\n",
    "        img_files = glob(file_path)\n",
    "        dest_dir = os.path.join(TBX_DEST_DIR,split_dir,class_dir)\n",
    "        #dest_file = os.path.join(dest_dir,os.path.basename(img_file))\n",
    "        #process_clahe({'filename':img_file,'dest_file':dest_file})\n",
    "        process_image_files(img_files,dest_dir)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "877a760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TBX_DIR_TEST = '/home/TBX11K/test'\n",
    "TBX_DEST_DIR_TEST = '/home/TBX11K/HEF/test'\n",
    "for split_dir in ['Montgomery','Shenzhen']:\n",
    "    for class_dir in ['health','sick','tb']:\n",
    "        file_path = os.path.join(TBX_DIR_TEST,split_dir,class_dir,'*.png')\n",
    "        img_files = glob(file_path)\n",
    "        dest_dir = os.path.join(TBX_DEST_DIR_TEST,split_dir,class_dir)\n",
    "        process_image_files(img_files,dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "203fcd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TBX_DIR_TEST = '/home/TBX11K/test'\n",
    "TBX_DEST_DIR_TEST = '/home/TBX11K/HEF/test'\n",
    "for split_dir in ['DA_DB']:\n",
    "    for class_dir in ['tb']:\n",
    "        file_path = os.path.join(TBX_DIR_TEST,split_dir,class_dir,'*.png')\n",
    "        img_files = glob(file_path)\n",
    "        dest_dir = os.path.join(TBX_DEST_DIR_TEST,split_dir,class_dir)\n",
    "        \n",
    "        #dest_file = os.path.join(dest_dir,os.path.basename(img_files[0]))\n",
    "        #print(img_files[0],dest_file)\n",
    "        #break\n",
    "        #process_clahe({'filename':img_file,'dest_file':dest_file})\n",
    "        process_image_files(img_files,dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aa469d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
